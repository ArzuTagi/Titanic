# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTT9gJZtBhUXnq4iyoWPq5E5BrSqbgVT
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.preprocessing import LabelEncoder

train_df=pd.read_csv('train.csv')

train_df.head()

test_df=pd.read_csv('test.csv')
test_df.head()

#EDA, Data preprocessing & Feature Engineering
#In our case we are required create model with dependent variable: "Survived" and independent variable such as "Sex" and "Pclass". That is why we are carry out our EDA on these variables.

train_df.head()

train_df.info()

test_df.info()

train_df.isnull().sum()

train_df.describe()

ax=sns.countplot(data=train_df, x ="Survived")
abs_values = train_df['Survived'].value_counts(ascending=False).values
ax.bar_label(container=ax.containers[0], labels=abs_values)
plt.show()

ax = sns.countplot(data=train_df, x='Sex')
abs_values = train_df['Sex'].value_counts(ascending=False).values
ax.bar_label(container=ax.containers[0], labels=abs_values)
plt.show()

ax = sns.countplot(data=train_df, x='Pclass')
abs_values = train_df['Pclass'].value_counts(ascending=True).values

ax.bar_label(container=ax.containers[0], labels=abs_values)
plt.show()

ax = sns.countplot(data=train_df, x='Sex', hue='Survived')
for label in ax.containers:
    ax.bar_label(label)
plt.show()

ax = sns.countplot(data=train_df, x='Sex', hue='Pclass')
for label in ax.containers:
    ax.bar_label(label)
plt.show()

ax = sns.countplot(data=train_df, x='Pclass', hue='Survived')
for label in ax.containers:
    ax.bar_label(label)
plt.show()

train_df.groupby('Sex').agg({'Survived': ['mean', 'count','sum']})

train_df.groupby('Pclass').agg({'Survived': ['mean', 'count','sum']})

train_df.groupby(['Sex', 'Pclass']).agg({'Survived': ['mean', 'count','sum']})

test_df['Sex'].value_counts()

test_df['Pclass'].value_counts()

ax = sns.countplot(data=test_df, x='Sex', hue='Pclass')
for label in ax.containers:
    ax.bar_label(label)
plt.show()

train_df[['Survived','Sex','Pclass']].isnull().sum()

train_df['Sex'] = LabelEncoder().fit_transform(train_df['Sex'])
train_df1=train_df.copy()
train_df1.head()

#test_df['Sex'] = LabelEncoder().fit_transform(test_df['Sex'])
#test_df1=test_df.copy()
#test_df1.head()

train_df1[['Sex']].info()

corr=train_df1.corr()
corr

sns.heatmap(corr, annot = True)

# According to the results we get the following insights about our data:
# 'Survived' and 'Sex' features are nominal categoric variables, 'Pclass' is ordinal categoric variable.
# There are not null values in 'Survived','Sex' and 'Pclass' variables.
# There are 577 men and 314 women in Titanic. 
#There are 184 peoples in Pclass_1 (122 men, 94 women), 216 peoples in Pclass_2 (108 men, 76 women) and 491 peoples in Pclass_3 (347 men, 144 women).
# Thus, majority of people are in Pclass_3.
# From 891 peoples 342 survived (109 men, 233 women), 549 not survuved (468 men, 81 women).
# 74.2 percent of women survived. 18.9 percent of men survived.
# About 63% of peoples survived in Pclass_1. Thus majority of people survived from Pclass_1.
# Most of the women survived, and the majority of the male died.

#From the above we can say that there is a relationship between {"survived" and "sex"}, {"survived" and "Pclass"}
# For proving it in a scientific way we must test it via statistical tests.

from scipy.stats import chi2_contingency
stat, p, dof, expected = chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))
alpha = 0.05
print("p value is " + str(p))
if p <= alpha:
    print('There is a relationship between "Survived" and "Sex" (reject H0)')
else:
   print('There is no relationship between "Survived" and "Sex" (H0 holds true)')

stat, p, dof, expected = chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))
alpha = 0.05
print("p value is " + str(p))
if p <= alpha:
    print('There is a relationship between "Survived" and "Pclass" (reject H0)')
else:
   print('There is no relationship between "Survived" and "Pclass" (H0 holds true)')

train_df1 = pd.get_dummies(train_df, columns=['Sex'])
train_df1.head()

test_df1 = pd.get_dummies(test_df, columns=['Sex'])
test_df1.head()

# According the test result we prove that there is a significant relationship between dependent and independent features.

# Question 3,4,5

import pandas as pd
from sklearn.tree import DecisionTreeClassifier,export_graphviz,export_text
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split,GridSearchCV, cross_validate,validation_curve
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib

y=train_df1["Survived"]
y

X=train_df1[['Sex_female','Sex_male','Pclass']]
X

#CART model
cart_model=DecisionTreeClassifier(random_state=1).fit(X,y)

y_pred_cart=cart_model.predict(X)
y_pred_cart

y_prob_cart= cart_model.predict_proba(X)[:,1]

y_prob_cart

print(classification_report(y,y_pred_cart))

roc_auc_score(y_train,y_prob_cart)

#Hold-out
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=45)

cart_model_tr= DecisionTreeClassifier(random_state=17).fit(X_train,y_train)

y_pred_cart_tr=cart_model.predict(X_train)
y_prob_cart_tr=cart_model.predict_proba(X_train)[:,1]

y_pred_cart_tr

y_prob_cart_tr

print(classification_report(y_train,y_pred_cart_tr))

roc_auc_score(y_train,y_prob_cart_tr)

y_pred_cart_te=cart_model.predict(X_test)
y_prob_cart_te=cart_model.predict_proba(X_test)[:,1]

print(classification_report(y_test,y_pred_cart_te))

roc_auc_score(y_test,y_prob_cart_te)

cart_model =DecisionTreeClassifier(random_state=17).fit(X,y)

cv_results_cart=cross_validate(cart_model, X, y, cv=5, scoring=["accuracy","f1","roc_auc"])

cv_results_cart['test_accuracy'].mean()

cv_results_cart['test_f1'].mean()

cv_results_cart['test_roc_auc'].mean()

#Hyperparameter Optimization

cart_params={'max_depth':range(1,20),"min_samples_split":range(2,26)}

cart_best_grid=GridSearchCV(cart_model,cart_params, cv=5,n_jobs=1,verbose=1).fit(X,y)

cart_best_grid.best_params_

cart_final = DecisionTreeClassifier(**cart_best_grid.best_params_,random_state=17).fit(X,y)
cart_final.get_params()

cart_final

cv_results = cross_validate(cart_final,X,y,cv=5,scoring=["accuracy","f1","roc_auc"])

cv_results

cv_results['test_accuracy'].mean()

cv_results['test_f1'].mean()

cv_results['test_roc_auc'].mean()

import graphviz
import pydotplus

def tree_graph(model, col_names, file_name):
    tree_str= export_graphviz(model, feature_names=col_names, filled=True, out_file=None)
    graph = pydotplus.graph_from_dot_data(tree_str)
    graph.write_png(file_name)

tree_graph(model=cart_final, col_names=X.columns, file_name="cart_final.png")

X_t=test_df1[['Sex_female','Sex_male','Pclass']]
X_t

predictions_cart = cart_final.predict(X_t)

predictions_cart

output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})
output.to_csv('submission_cart.csv', index=False)
print("Your submission was successfully saved!")

df=pd.read_csv('submission.csv')
df.head()

df['Survived'].value_counts()

#KNN

from sklearn.metrics import classification_report, roc_auc_score

from sklearn.model_selection import GridSearchCV, cross_validate

from sklearn.neighbors import KNeighborsClassifier

from sklearn.preprocessing import StandardScaler

X,y

knn_model= KNeighborsClassifier().fit(X,y)

y_pred_knn=knn_model.predict(X)

y_pred_knn

y_prob_knn=knn_model.predict_proba(X)[:,1]

y_prob_knn

print(classification_report(y,y_pred_knn))

roc_auc_score(y,y_prob_knn)

cv_results_knn= cross_validate(knn_model,X,y,cv=5,scoring=["accuracy","f1","roc_auc"])
cv_results_knn

cv_results_knn['test_accuracy'].mean()

cv_results_knn['test_f1'].mean()

cv_results_knn['test_roc_auc'].mean()

knn_model.get_params()

## Hyperparameter optimazation

knn_model = KNeighborsClassifier()

knn_params = {"n_neighbors":range(2,50)}

knn_gs_best = GridSearchCV(knn_model,knn_params,cv=5,n_jobs=1,verbose=1).fit(X,y)

knn_gs_best

knn_gs_best.best_params_

knn_final_model = knn_model.set_params(**knn_gs_best.best_params_).fit(X,y)
knn_final_model

cv_results_knn_final = cross_validate(knn_final_model,X,y,cv=5,scoring=["accuracy","f1","roc_auc"])
cv_results_knn_final

cv_results_knn_final['test_accuracy'].mean()

cv_results_knn_final['test_f1'].mean()

cv_results_knn_final['test_roc_auc'].mean()

predictions_knn = knn_final_model.predict(X_t)

output = pd.DataFrame({'PassengerId': test_df1.PassengerId, 'Pclass': test_df1.Pclass, 'Sex_female': test_df1.Sex_female, 'Sex_male': test_df1.Sex_male, 'Survived': predictions_knn})
output.to_csv('test_df_knn.csv', index=False)
print("Your submission was successfully saved!")

df_knn_test=pd.read_csv('test_df_knn.csv')
df_knn_test.head()

df_knn_test['Survived'].value_counts()

#Accepted KNN  model scores:  test_accuracy: 0.787; 
#                             test_f1: 0.709
#                             test_roc_auc=0.825

#logistic regression model

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)

log_reg = LogisticRegression()

log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)

print("Accuracy: ", log_reg.score(X_test, y_test))

y_prob_log=log_reg.predict_proba(X)[:,1]
y_prob_log

roc_auc_score(y,y_prob_log)

cv_results_log = cross_validate(log_reg,X,y,cv=5,scoring=["accuracy","f1","roc_auc"])
cv_results_log

cv_results_log['test_accuracy'].mean()

cv_results_log['test_f1'].mean()

cv_results_log['test_roc_auc'].mean()

predictions_log = log_reg.predict(X_t)

output = pd.DataFrame({'PassengerId': test_df1.PassengerId, 'Pclass': test_df1.Pclass, 'Sex_female': test_df1.Sex_female, 'Sex_male': test_df1.Sex_male, 'Survived': predictions_log})
output.to_csv('test_df_log.csv', index=False)
print("Your submission was successfully saved!")

df_log=pd.read_csv('test_df_log.csv')
df_log.head()

df_log['Survived'].value_counts()

#Logistic model scores:  test_accuracy: 0.787; 
#                        test_f1: 0.709
#                        test_roc_auc=0.833

# After the models investigating we can conculude that KNN and Logistic regression model is more appropriate for the creating model based on features 'Sex' and 'Pclass'